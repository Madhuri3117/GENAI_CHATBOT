{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "10594a72-1771-44b1-a8c7-0ca83f872f1b",
      "metadata": {
        "id": "10594a72-1771-44b1-a8c7-0ca83f872f1b"
      },
      "outputs": [],
      "source": [
        "!pip install pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bc614b9-7cc4-4d67-9abb-1b0f1250c9e0",
      "metadata": {
        "id": "7bc614b9-7cc4-4d67-9abb-1b0f1250c9e0",
        "outputId": "293ff9cf-991a-4d77-9339-6b7d30eb0ced"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "\n",
        "uri = \"mongodb+srv://username:pwd@cluster5.dkdim.mongodb.net/?retryWrites=true&w=majority&appName=Cluster5\"\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c025e7d9-420a-4da8-bdf6-2236d0b5fd21",
      "metadata": {
        "id": "c025e7d9-420a-4da8-bdf6-2236d0b5fd21"
      },
      "outputs": [],
      "source": [
        "Install Google's Generative AI SDK\n",
        "!pip install google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "65e5a651-c11a-4fa4-96a6-9ff7566a4c2c",
      "metadata": {
        "id": "65e5a651-c11a-4fa4-96a6-9ff7566a4c2c"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade streamlit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "063a4642-1d3a-4819-bb41-6000e6d20162",
      "metadata": {
        "id": "063a4642-1d3a-4819-bb41-6000e6d20162"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d15f15-ecec-467f-87c5-fec0770517d0",
      "metadata": {
        "id": "e8d15f15-ecec-467f-87c5-fec0770517d0",
        "outputId": "b49b6590-9231-4109-eb3a-c80928bcb38b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google Generative AI and Streamlit installed successfully!\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai\n",
        "import streamlit\n",
        "print(\"Google Generative AI and Streamlit installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21b21fb-aa22-48a0-9991-3019092704c2",
      "metadata": {
        "id": "e21b21fb-aa22-48a0-9991-3019092704c2"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b686851-3198-419c-9f4b-c0ce2127f907",
      "metadata": {
        "id": "4b686851-3198-419c-9f4b-c0ce2127f907"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key=\"YOUR_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab6b716-2f68-46e5-a794-9dcf1ae1119c",
      "metadata": {
        "id": "eab6b716-2f68-46e5-a794-9dcf1ae1119c"
      },
      "outputs": [],
      "source": [
        "def get_embedding(data):\n",
        "    \"\"\"Generates vector embeddings using Google's Gemini API.\"\"\"\n",
        "    model = genai.GenerativeModel(\"gemini-pro\")  # Use Gemini Pro\n",
        "    response = model.generate_content(data_type=\"text\",content=data)\n",
        "    return response['embedding']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "331e02c1-9ccd-4aa1-9cae-508fd479e2f8",
      "metadata": {
        "id": "331e02c1-9ccd-4aa1-9cae-508fd479e2f8"
      },
      "outputs": [],
      "source": [
        "!pip install pymupdf4llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ba626426-f71e-4466-96d8-ff80e7ee235a",
      "metadata": {
        "id": "ba626426-f71e-4466-96d8-ff80e7ee235a"
      },
      "outputs": [],
      "source": [
        "import pymupdf4llm\n",
        "\n",
        "md_text = pymupdf4llm.to_markdown(\"C:/ragqa/AR_24733_KPITTECH_2023_2024_30072024211522.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecdba38b-f536-40f4-a64e-14aa3478293d",
      "metadata": {
        "id": "ecdba38b-f536-40f4-a64e-14aa3478293d"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import Document\n",
        "# from langchain.text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Assuming 'md_text' is your plain string of text\n",
        "document = Document(page_content=md_text)\n",
        "\n",
        "# Now you can split the documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=20)\n",
        "documents = text_splitter.split_documents([document])\n",
        "\n",
        "# Now 'documents' will be split correctly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7360809c-06c6-4e12-a93f-b571f147edc4",
      "metadata": {
        "id": "7360809c-06c6-4e12-a93f-b571f147edc4"
      },
      "outputs": [],
      "source": [
        "def get_embedding(text):\n",
        "    \"\"\"Generates vector embeddings using Google's Gemini API.\"\"\"\n",
        "    response = genai.embed_content(model=\"models/embedding-001\", content=text)  # Correct method\n",
        "    return response['embedding']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26556527-6e6e-4bf5-8798-e639ce185d68",
      "metadata": {
        "id": "26556527-6e6e-4bf5-8798-e639ce185d68"
      },
      "outputs": [],
      "source": [
        "# Prepare documents for insertion with metadata\n",
        "docs_to_insert = [{\n",
        "    \"text\": doc.page_content,   # Extracted text\n",
        "    \"embedding\": get_embedding(doc.page_content),  # Google Gemini embeddings\n",
        "    \"metadata\": {\n",
        "        \"page_number\": doc.metadata.get(\"page\", None),  # Page number from PDF\n",
        "        \"source\": \"md_text\"  # Example document source\n",
        "    }\n",
        "} for doc in documents]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36c3bc67-53e6-4a9b-9bd4-422bda5abda9",
      "metadata": {
        "id": "36c3bc67-53e6-4a9b-9bd4-422bda5abda9"
      },
      "outputs": [],
      "source": [
        "import pymongo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deaf8828-221f-4413-8290-4c40bacb82bd",
      "metadata": {
        "id": "deaf8828-221f-4413-8290-4c40bacb82bd"
      },
      "outputs": [],
      "source": [
        "client = pymongo.MongoClient(uri)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30dd7400-8d70-41d4-9b59-bb198861c4e6",
      "metadata": {
        "id": "30dd7400-8d70-41d4-9b59-bb198861c4e6",
        "outputId": "d5cc584c-4c63-48e9-f235-575b3558a346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected successfully!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Connected successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Connection failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ebbea9-65f0-48af-99aa-f0b3fd7dac7e",
      "metadata": {
        "id": "65ebbea9-65f0-48af-99aa-f0b3fd7dac7e"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "uri = \"mongodb+srv://username:pwd@cluster5.dkdim.mongodb.net/?retryWrites=true&w=majority&appName=Cluster5\"\n",
        "# Connect to your Atlas cluster\n",
        "client = MongoClient(uri)\n",
        "collection = client[\"RAGDB\"][\"ragdemo_googleapi\"]\n",
        "\n",
        "# Insert documents into the collection\n",
        "result = collection.insert_many(docs_to_insert)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8723289-1b5a-4683-a17c-0b3cd354661b",
      "metadata": {
        "id": "c8723289-1b5a-4683-a17c-0b3cd354661b",
        "outputId": "b8da0933-bf59-4cce-83bd-5e7d8f59ef9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Polling to check if the index is ready. This may take up to a minute.\n",
            "GoogleEmbed_Index is ready for querying.\n"
          ]
        }
      ],
      "source": [
        "from pymongo.operations import SearchIndexModel\n",
        "import time\n",
        "\n",
        "# Create your index model, then create the search index\n",
        "index_name=\"GoogleEmbed_Index\"\n",
        "search_index_model = SearchIndexModel(\n",
        "  definition = {\n",
        "    \"fields\": [\n",
        "      {\n",
        "        \"type\": \"vector\",\n",
        "        \"numDimensions\": 768,\n",
        "        \"path\": \"embedding\",\n",
        "        \"similarity\": \"cosine\"\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  name = index_name,\n",
        "  type = \"vectorSearch\"\n",
        ")\n",
        "collection.create_search_index(model=search_index_model)\n",
        "\n",
        "# Wait for initial sync to complete\n",
        "print(\"Polling to check if the index is ready. This may take up to a minute.\")\n",
        "predicate=None\n",
        "if predicate is None:\n",
        "   predicate = lambda index: index.get(\"queryable\") is True\n",
        "\n",
        "while True:\n",
        "   indices = list(collection.list_search_indexes(index_name))\n",
        "   if len(indices) and predicate(indices[0]):\n",
        "      break\n",
        "   time.sleep(5)\n",
        "print(index_name + \" is ready for querying.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fb2e4eb8-31c9-458a-a377-37e981f56386",
      "metadata": {
        "id": "fb2e4eb8-31c9-458a-a377-37e981f56386"
      },
      "outputs": [],
      "source": [
        "# Define a function to run vector search queries\n",
        "def get_query_results(query):\n",
        "  \"\"\"Gets results from a vector search query.\"\"\"\n",
        "\n",
        "  query_embedding = get_embedding(query)\n",
        "  pipeline = [\n",
        "      {\n",
        "            \"$vectorSearch\": {\n",
        "              \"index\": \"GoogleEmbed_Index\",\n",
        "              \"queryVector\": query_embedding,\n",
        "              \"path\": \"embedding\",\n",
        "              \"exact\": True,\n",
        "              \"limit\": 5\n",
        "            }\n",
        "      }, {\n",
        "            \"$project\": {\n",
        "              \"_id\": 0,\n",
        "              \"text\": 1\n",
        "         }\n",
        "      }\n",
        "  ]\n",
        "\n",
        "  results = collection.aggregate(pipeline)\n",
        "\n",
        "  array_of_results = []\n",
        "  for doc in results:\n",
        "      array_of_results.append(doc)\n",
        "  return array_of_results\n",
        "\n",
        "# Test the function with a sample query\n",
        "import pprint\n",
        "pprint.pprint(get_query_results(\"AI technology\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3f4bc33b-55d2-4599-bcb7-fd3224a30132",
      "metadata": {
        "id": "3f4bc33b-55d2-4599-bcb7-fd3224a30132"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Google Gemini API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_API_KEY\"\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "# Specify search query, retrieve relevant documents, and convert to string\n",
        "query = \"How We Make KPIT #TheBestPlaceToGrow?\"\n",
        "context_docs = get_query_results(query)\n",
        "context_string = \" \".join([doc[\"text\"] for doc in context_docs])\n",
        "\n",
        "# Construct prompt for the LLM using the retrieved documents as context\n",
        "prompt = f\"\"\"Use the following pieces of context to answer the question at the end.\n",
        "    {context_string}\n",
        "    Question: {query}\n",
        "\"\"\"\n",
        "\n",
        "# Load the Gemini model\n",
        "model = genai.GenerativeModel(\"gemini-pro\")  # Use Gemini Pro for text generation\n",
        "\n",
        "# Generate response from Gemini API\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Print the model output\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e231ed9c-bc2a-4e1f-869a-de16b4a86dcc",
      "metadata": {
        "id": "e231ed9c-bc2a-4e1f-869a-de16b4a86dcc"
      },
      "outputs": [],
      "source": [
        "query = \"KPITTECH\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d24a3d-266b-48a4-ac1a-e23aeb04a02d",
      "metadata": {
        "id": "90d24a3d-266b-48a4-ac1a-e23aeb04a02d",
        "outputId": "b6b5df4c-f11e-484a-bfb1-f41fd5a6fb0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "By focusing on training and education, KPIT has been able to fulfill 90% of leadership roles internally by high performers, and over 75% of promotions being awarded at lead roles and above.\n"
          ]
        }
      ],
      "source": [
        "model = genai.GenerativeModel(\"gemini-pro\")  # Use Gemini Pro for text generation\n",
        "\n",
        "# Generate response from Gemini API\n",
        "response = model.generate_content(prompt)\n",
        "\n",
        "# Print the model output\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dae59a56-1a19-45ae-b121-c91a32b753e0",
      "metadata": {
        "id": "dae59a56-1a19-45ae-b121-c91a32b753e0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4bedb7-cad0-4b90-aff1-1ca393507ef7",
      "metadata": {
        "id": "3f4bedb7-cad0-4b90-aff1-1ca393507ef7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "testhuggingface",
      "language": "python",
      "name": "ragqa"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}